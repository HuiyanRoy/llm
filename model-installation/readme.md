# Running IBM’s 128K Granite Model on Mac – A Guide for the Curious
When exploring the powerful capabilities of large language models (LLMs), most developers and tinkerers face a common barrier: deployment complexity. What if running high-context-window models like IBM's Granite 128K on your local MacBook could be easy, fast, and efficient?

That’s exactly the kind of challenge I love solving — and it’s what inspired me to write this guide

## Why Granite?

IBM’s Granite models are a powerful family of LLMs designed for tasks like code generation, natural language understanding, and more. They offer up to 128K context window, they offer immense potential for real-world applications requiring long-term memory—whether you're analyzing huge documents, generating technical content, or simulating agents with deep context.

But where do you start if you want to try it out locally?

## Why Mac + Ollama or Llama.cpp etc.?
A lot of LLM tooling is Linux-first or cloud-focused, but Mac users deserve some local LLM love too! With Ollama, llama.cpp etc, running LLMs natively on macOS becomes surprisingly seamless.
- fast setup
- good performance
- great developer experience

That’s why I built the guide for Mac users who want to experiment locally — without wrangling Docker images or complex model converters.

## Join the Conversation

I’m always curious how others are using these tools. If you tried the guide or improved on it—I’d love to hear from you! Feel free to fork the repo, drop an issue, or share your setup.

