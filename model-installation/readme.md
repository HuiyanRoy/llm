# Running IBM’s 128K Granite Model on Mac with Ollama – A Guide for the Curious
When exploring the powerful capabilities of large language models (LLMs), most developers and tinkerers face a common barrier: deployment complexity. What if running high-context-window models like IBM's Granite 128K on your local MacBook could be easy, fast, and efficient?

That’s exactly the kind of challenge I love solving—and it’s what inspired me to write this guide

## Why Granite?

IBM’s Granite models are a powerful family of LLMs designed for tasks like code generation, natural language understanding, and more. With a 128K context window, they offer immense potential for real-world applications requiring long-term memory—whether you're analyzing huge documents, generating technical content, or simulating agents with deep context.

But where do you start if you want to try it out locally?

## Why Mac + Ollama?
A lot of LLM tooling is Linux-first or cloud-focused, but Mac users deserve some local LLM love too! With Ollama, running LLMs natively on macOS becomes surprisingly seamless.
- fast setup
- good performance
- great developer experience

That’s why I built the guide for Mac users who want to experiment locally—without wrangling Docker images or complex model converters.

## What the Guide Covers
Here’s what you’ll find in the step-by-step walkthrough:

- Installing Ollama on your Mac using Homebrew
- Starting the Ollama server correctly
- Pulling and running IBM’s Granite models, including granite3.2:8b
- Tips for running with a 128K context window smoothly
- And a few notes for navigating quirks along the way

## Join the Conversation

I’m always curious how others are using these tools. If you tried the guide or improved on it—I’d love to hear from you! Feel free to fork the repo, drop an issue, or share your setup.

