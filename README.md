# LLM Experiments: Installation, Configuration, inferrencing and Fine-Tuning  

## Overview  
This repository documents my experiments with Large Language Models (LLMs), focusing on IBM Granite models and other open-source models. It aims to share insights, best practices, and practical guides to help developers, researchers, and AI practitioners deploy and optimize LLMs for various applications.  

Unless otherwise specified, all instructions are designed for **MacBook Pro M1 Max** environments with:  
- **10 CPU cores**  
- **32-core GPU**  

## Contents  
- [Model-Installation](#model-installation)  
- [Fine-Tuning](#fine-tuning)  
- [Performance Optimization](#performance-optimization)  
- [Use Cases & Applications](#use-cases--applications)  
- [Resources](#resources)  
- [Contributing](#contributing)  

## Getting Started  

Model Installation

    IBM Granite
    Hugging Face Transformers
    Llama, Falcon, Mistral setup

Fine-Tuning

    Using InstructLab for fine-tuning 
    Data preprocessing
    Hyperparameter tuning

Performance Optimization

    Running on Apple M1 GPU
    Mixed precision (fp16/bf16) and quantized models
    Memory optimization strategies

Use Cases & Applications

    Code generation
    Natural language understanding (NLU)
    Chatbots & AI assistants

Resources

    IBM Granite Model Docs
    Hugging Face Transformers

Contributing

Contributions are welcome! Feel free to submit issues, improvements, and suggestions.
